example:  Example: 
{
    "p_id": 6,
    "context": "禅意歌者刘珂矣《一袖云》中诉知己…绵柔纯净的女声，将心中的万水千山尽意勾勒于这清素画音中",
    "raw_context": null,
    "text_word": [
        "禅意",
        "歌者",
        "刘珂",
        "矣",
        "《",
        "一袖",
        "云",
        "》",
        "中诉",
        "知己",
        "…",
        "绵柔",
        "纯净",
        "的",
        "女声",
        "，",
        "将",
        "心中",
        "的",
        "万水千山",
        "尽意",
        "勾勒",
        "于",
        "这",
        "清素",
        "画音",
        "中"
    ],
    "bert_tokens": null,
    "entity_list": [
        "一袖云",
        "刘珂矣"
    ],
    "gold_answer": [
        [
            "一袖云",
            "歌手",
            "刘珂矣"
        ]
    ]
}
batch_char_ids: shape= torch.Size([1, 44]) 
 tensor([[1944,  445,   45,   50,  166, 1926, 3351,    7,   15, 1980,  345,    8,
           17, 1586,  395,  443, 1671, 1910, 1297, 1040, 1906,    4,   78,  544,
            3,  364,  156,   17,    4,  385,  330,  590,  127, 1184,  445, 2615,
         1110,   20,  123,  350, 1001,  371,  224,   17]])
batch_word_ids: shape= torch.Size([1, 44]) 
 tensor([[21862, 21862, 10142, 10142, 15275, 15275,  9236,     3,     0,     0,
          1392,     4,     0,     0,  8339,  8339,  1036,     0,     0, 12910,
         12910,     2,  7602,  7602,     1,   131,  1419,  1419,     2, 15857,
         15857, 15857, 15857,     0,     0, 17142, 17142,    10,    96,     0,
             0,     0,     0,    22]])
batch_ent_labels: shape= torch.Size([1, 44]) 
 tensor([[0, 0, 0, 0, 1, 2, 2, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
batch_rel_labels: shape= torch.Size([1, 44, 49, 44]) 
 tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
(4, <class 'tuple'>)
mhs with w2v
using BertAdam
example:  Example: 
{
    "p_id": 5,
    "context": "《逐风行》是百度文学旗下纵横中文网签约作家清水秋风创作的一部东方玄幻小说，小说已于2014-04-28正式发布",
    "raw_context": null,
    "text_word": [
        "《",
        "逐",
        "风行",
        "》",
        "是",
        "百度",
        "文学",
        "旗下",
        "纵横",
        "中文网",
        "签约",
        "作家",
        "清水",
        "秋风",
        "创作",
        "的",
        "一部",
        "东方",
        "玄幻",
        "小说",
        "，",
        "小说",
        "已于",
        "2014",
        "-",
        "04",
        "-",
        "28",
        "正式",
        "发布"
    ],
    "bert_tokens": null,
    "entity_list": [
        "逐风行",
        "纵横中文网",
        "逐风行",
        "清水秋风"
    ],
    "gold_answer": [
        [
            "逐风行",
            "连载网站",
            "纵横中文网"
        ],
        [
            "逐风行",
            "作者",
            "清水秋风"
        ]
    ]
}
batch_char_ids: shape= torch.Size([1, 55]) 
 tensor([[   7, 1650,  268,  105,    8,    6,  483,  349,   47,   19,  787,  194,
          862,  749,   17,   47,   56,  928,  447,   14,   52,  350,  330,  702,
          268,  126,   14,    4,   15,   74,  109,  206,  861,  556,   27,   43,
            3,   27,   43,  407,   20,   12,    9,    5,   46,  130,    9,   46,
          130,   12,   28,  258,  420,   73,  315]])
batch_word_ids: shape= torch.Size([1, 55]) 
 tensor([[    3,     0, 14726, 14726,     4,     5,   611,   611,   284,   284,
           421,   421,   344,   344,   122,   122,   122,   412,   412,   244,
           244,  6724,  6724, 13304, 13304,    57,    57,     2,     0,     0,
           489,   489,   704,   704,    41,    41,     1,    41,    41,   628,
           628,   133,   133,   133,   133,    36,   549,   549,    36,   425,
           425,   208,   208,   393,   393]])
batch_ent_labels: shape= torch.Size([1, 55]) 
 tensor([[0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2,
         2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0]])
batch_rel_labels: shape= torch.Size([1, 55, 49, 55]) 
 tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
selection_logits.shape =  torch.Size([1, 55, 49, 55])
selection_logits.shape =  torch.Size([1, 55, 49, 55])
mask.shape =  torch.Size([1, 55])
mask.unsqueeze(2).shape =  torch.Size([1, 55, 1])
mask.unsqueeze(1).shape =  torch.Size([1, 1, 55])
product:  torch.Size([1, 55, 1, 55])
selection_mask.shape =  torch.Size([1, 55, 49, 55])
selection_loss.shape =  torch.Size([1, 55, 49, 55])
selection_loss.shape =  torch.Size([148225])
selection_loss =  tensor(122523.8359, device='cuda:0', grad_fn=<SumBackward0>)
selection_loss =  tensor(2227.7061, device='cuda:0', grad_fn=<DivBackward0>)
selection_mask.sum =  tensor(55, device='cuda:0')
example:  Example: 
{
    "p_id": 6,
    "context": "禅意歌者刘珂矣《一袖云》中诉知己…绵柔纯净的女声，将心中的万水千山尽意勾勒于这清素画音中",
    "raw_context": "禅意歌者刘珂矣《一袖云》中诉知己…绵柔纯净的女声，将心中的万水千山尽意勾勒于这清素画音中",
    "text_word": [
        "禅意",
        "歌者",
        "刘珂",
        "矣",
        "《",
        "一袖",
        "云",
        "》",
        "中诉",
        "知己",
        "…",
        "绵柔",
        "纯净",
        "的",
        "女声",
        "，",
        "将",
        "心中",
        "的",
        "万水千山",
        "尽意",
        "勾勒",
        "于",
        "这",
        "清素",
        "画音",
        "中"
    ],
    "bert_tokens": null,
    "entity_list": [
        "一袖云",
        "刘珂矣"
    ],
    "gold_answer": [
        [
            "一袖云",
            "歌手",
            "刘珂矣"
        ]
    ]
}
batch_char_ids: shape= torch.Size([1, 44]) 
 tensor([[1944,  445,   45,   50,  166, 1926, 3351,    7,   15, 1980,  345,    8,
           17, 1586,  395,  443, 1671, 1910, 1297, 1040, 1906,    4,   78,  544,
            3,  364,  156,   17,    4,  385,  330,  590,  127, 1184,  445, 2615,
         1110,   20,  123,  350, 1001,  371,  224,   17]])
batch_word_ids: shape= torch.Size([1, 44]) 
 tensor([[21862, 21862, 10142, 10142, 15275, 15275,  9236,     3,     0,     0,
          1392,     4,     0,     0,  8339,  8339,  1036,     0,     0, 12910,
         12910,     2,  7602,  7602,     1,   131,  1419,  1419,     2, 15857,
         15857, 15857, 15857,     0,     0, 17142, 17142,    10,    96,     0,
             0,     0,     0,    22]])
batch_ent_labels: shape= torch.Size([1, 44]) 
 tensor([[0, 0, 0, 0, 1, 2, 2, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
batch_rel_labels: shape= torch.Size([1, 44, 49, 44]) 
 tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
selection_logits.shape =  torch.Size([1, 44, 49, 44])
selection_logits.shape =  torch.Size([1, 44, 49, 44])
mask.shape =  torch.Size([1, 44])
mask.unsqueeze(2).shape =  torch.Size([1, 44, 1])
mask.unsqueeze(1).shape =  torch.Size([1, 1, 44])
product:  torch.Size([1, 44, 1, 44])
selection_mask.shape =  torch.Size([1, 44, 49, 44])
selection_loss.shape =  torch.Size([1, 44, 49, 44])
selection_loss.shape =  torch.Size([94864])
selection_loss =  tensor(78949.3281, device='cuda:0', grad_fn=<SumBackward0>)
selection_loss =  tensor(1794.3029, device='cuda:0', grad_fn=<DivBackward0>)
selection_mask.sum =  tensor(44, device='cuda:0')
example:  Example: 
{
    "p_id": 3,
    "context": "丝角蝗科，oedipodidae，昆虫纲直翅目蝗总科的一个科",
    "raw_context": null,
    "text_word": [
        "丝角",
        "蝗科",
        "，",
        "oedipodidae",
        "，",
        "昆虫",
        "纲",
        "直翅目",
        "蝗",
        "总科",
        "的",
        "一个",
        "科"
    ],
    "bert_tokens": null,
    "entity_list": [
        "丝角蝗科",
        "直翅目"
    ],
    "gold_answer": [
        [
            "丝角蝗科",
            "目",
            "直翅目"
        ]
    ]
}
batch_char_ids: shape= torch.Size([1, 30]) 
 tensor([[ 955,  366, 2352,   64,    3,   77,   57,  225,   69,  245,   77,  225,
           69,  225,   49,   57,    3,  744,  634,  530,  482,  559,  119, 2352,
          288,   64,    4,   15,   79,   64]])
batch_word_ids: shape= torch.Size([1, 30]) 
 tensor([[   0,    0,    0,    0,    1,    0,    0,    0,    0,    0,    0,    0,
            0,    0,    0,    0,    1,  413,  413,  278, 3480, 3480, 3480, 5232,
            0,    0,    2,   70,   70,  148]])
batch_ent_labels: shape= torch.Size([1, 30]) 
 tensor([[1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0,
         0, 0, 0, 0, 0, 0]])
batch_rel_labels: shape= torch.Size([1, 30, 49, 30]) 
 tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
selection_logits.shape =  torch.Size([1, 30, 49, 30])
selection_logits.shape =  torch.Size([1, 30, 49, 30])
mask.shape =  torch.Size([1, 30])
mask.unsqueeze(2).shape =  torch.Size([1, 30, 1])
mask.unsqueeze(1).shape =  torch.Size([1, 1, 30])
product:  torch.Size([1, 30, 1, 30])
selection_mask.shape =  torch.Size([1, 30, 49, 30])
selection_loss.shape =  torch.Size([1, 30, 49, 30])
selection_loss.shape =  torch.Size([44100])
selection_loss =  tensor(34479.0078, device='cuda:0', grad_fn=<SumBackward0>)
selection_loss =  tensor(1149.3003, device='cuda:0', grad_fn=<DivBackward0>)
selection_mask.sum =  tensor(30, device='cuda:0')
example:  Example: 
{
    "p_id": 2,
    "context": "茶树茶网蝽，stephanitischinensisdrake，属半翅目网蝽科冠网椿属的一种昆虫",
    "raw_context": null,
    "text_word": [
        "茶树",
        "茶网",
        "蝽",
        "，",
        "stephanitischinensisdrake",
        "，",
        "属",
        "半翅目",
        "网",
        "蝽",
        "科冠",
        "网椿属",
        "的",
        "一种",
        "昆虫"
    ],
    "bert_tokens": null,
    "entity_list": [
        "茶树茶网蝽",
        "半翅目"
    ],
    "gold_answer": [
        [
            "茶树茶网蝽",
            "目",
            "半翅目"
        ]
    ]
}
batch_char_ids: shape= torch.Size([1, 48]) 
 tensor([[1374,  833, 1374,   56, 2060,    3,   85,  114,   57,  245,  209,   49,
           80,   69,  114,   69,   85,  122,  209,   69,   80,   57,   80,   85,
           69,   85,  225,   92,   49,  148,   57,    3,  190,  793,  559,  119,
           56, 2060,   64,  770,   56, 3285,  190,    4,   15,  249,  744,  634]])
batch_word_ids: shape= torch.Size([1, 48]) 
 tensor([[26475, 26475,     0,     0,  1679,     1,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     1,    98,  2642,  2642,  2642,   967,  1679,     0,     0,
             0,     0,     0,     2,     0,     0,   413,   413]])
batch_ent_labels: shape= torch.Size([1, 48]) 
 tensor([[1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
batch_rel_labels: shape= torch.Size([1, 48, 49, 48]) 
 tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
selection_logits.shape =  torch.Size([1, 48, 49, 48])
selection_logits.shape =  torch.Size([1, 48, 49, 48])
mask.shape =  torch.Size([1, 48])
mask.unsqueeze(2).shape =  torch.Size([1, 48, 1])
mask.unsqueeze(1).shape =  torch.Size([1, 1, 48])
product:  torch.Size([1, 48, 1, 48])
selection_mask.shape =  torch.Size([1, 48, 49, 48])
selection_loss.shape =  torch.Size([1, 48, 49, 48])
selection_loss.shape =  torch.Size([112896])
selection_loss =  tensor(85107.6406, device='cuda:0', grad_fn=<SumBackward0>)
selection_loss =  tensor(1773.0758, device='cuda:0', grad_fn=<DivBackward0>)
selection_mask.sum =  tensor(48, device='cuda:0')
example:  Example: 
{
    "p_id": 4,
    "context": "爱德华·尼科·埃尔南迪斯（1986-），是一位身高只有70公分哥伦比亚男子，体重10公斤，只比随身行李高一些，2010年获吉尼斯世界纪录正式认证，成为全球当今最矮的成年男人",
    "raw_context": null,
    "text_word": [
        "爱德华",
        "·",
        "尼科",
        "·",
        "埃尔南",
        "迪斯",
        "（",
        "1986",
        "-",
        "）",
        "，",
        "是",
        "一位",
        "身高",
        "只有",
        "70",
        "公分",
        "哥伦比亚",
        "男子",
        "，",
        "体重",
        "10",
        "公斤",
        "，",
        "只",
        "比",
        "随身行李",
        "高",
        "一些",
        "，",
        "2010",
        "年",
        "获",
        "吉尼斯世界纪录",
        "正式",
        "认证",
        "，",
        "成为",
        "全球",
        "当今",
        "最矮",
        "的",
        "成年",
        "男人"
    ],
    "bert_tokens": null,
    "entity_list": [
        "爱德华·尼科·埃尔南迪斯",
        "70公分",
        "爱德华·尼科·埃尔南迪斯",
        "1986",
        "爱德华·尼科·埃尔南迪斯",
        "哥伦比亚"
    ],
    "gold_answer": [
        [
            "爱德华·尼科·埃尔南迪斯",
            "身高",
            "70公分"
        ],
        [
            "爱德华·尼科·埃尔南迪斯",
            "出生日期",
            "1986"
        ],
        [
            "爱德华·尼科·埃尔南迪斯",
            "国籍",
            "哥伦比亚"
        ]
    ]
}
batch_char_ids: shape= torch.Size([1, 86]) 
 tensor([[  91,  219,  134,   83,  488,   64,   83, 1233,  204,  104,  688,  163,
           54,    5,   13,   28,   34,  130,   55,    3,    6,   15,  153,  232,
          103,  591,   31,   29,    9,   38,  187,  734,  493,  415,  213,   93,
           66,    3,  254,  234,    5,    9,   38, 1347,    3,  591,  415,  791,
          232,  105,  101,  103,   15,  721,    3,   12,    9,    5,    9,   10,
          376,  537,  488,  163,  189,  351,  458,  181,  258,  420,  711,  959,
            3,   58,   51,  222,  186,  255,  391,  161, 3095,    4,   58,   10,
           93,   18]])
batch_word_ids: shape= torch.Size([1, 86]) 
 tensor([[ 4821,  4821,  4821,    21, 28463, 28463,    21, 41998, 41998, 41998,
         37365, 37365,    14,   379,   379,   379,   379,    36,    15,     1,
             5,     0,     0,   130,   130,   791,   791,  1707,  1707,  4468,
          4468,  3039,  3039,  3039,  3039,  1741,  1741,     1,   207,   207,
            67,    67,   724,   724,     1,   750,   627,     0,     0,     0,
             0,   347,   815,   815,     1,   114,   114,   114,   114,     7,
           342, 26476, 26476, 26476, 26476, 26476, 26476, 26476,   208,   208,
          3091,  3091,     1,   169,   169,   632,   632,  7601,  7601, 37366,
         37366,     2,  8026,  8026,   732,   732]])
batch_ent_labels: shape= torch.Size([1, 86]) 
 tensor([[1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 1, 2, 2, 2, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
batch_rel_labels: shape= torch.Size([1, 86, 49, 86]) 
 tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
selection_logits.shape =  torch.Size([1, 86, 49, 86])
selection_logits.shape =  torch.Size([1, 86, 49, 86])
mask.shape =  torch.Size([1, 86])
mask.unsqueeze(2).shape =  torch.Size([1, 86, 1])
mask.unsqueeze(1).shape =  torch.Size([1, 1, 86])
product:  torch.Size([1, 86, 1, 86])
selection_mask.shape =  torch.Size([1, 86, 49, 86])
selection_loss.shape =  torch.Size([1, 86, 49, 86])
selection_loss.shape =  torch.Size([362404])
selection_loss =  tensor(260882.5469, device='cuda:0', grad_fn=<SumBackward0>)
selection_loss =  tensor(3033.5181, device='cuda:0', grad_fn=<DivBackward0>)
selection_mask.sum =  tensor(86, device='cuda:0')
example:  Example: 
{
    "p_id": 1,
    "context": "如何演好自己的角色，请读《演员自我修养》《喜剧之王》周星驰崛起于穷困潦倒之中的独门秘笈",
    "raw_context": null,
    "text_word": [
        "如何",
        "演",
        "好",
        "自己",
        "的",
        "角色",
        "，",
        "请读",
        "《",
        "演员",
        "自我",
        "修养",
        "》",
        "《",
        "喜剧之王",
        "》",
        "周星驰",
        "崛起",
        "于",
        "穷困潦倒",
        "之中",
        "的",
        "独门",
        "秘笈"
    ],
    "bert_tokens": null,
    "entity_list": [
        "喜剧之王",
        "周星驰"
    ],
    "gold_answer": [
        [
            "喜剧之王",
            "主演",
            "周星驰"
        ]
    ]
}
batch_char_ids: shape= torch.Size([1, 43]) 
 tensor([[ 312,  481,   23,  246,  125,  443,    4,  366,  347,    3, 1039,  563,
            7,   23,   95,  125,  106,  675,  947,    8,    7,  338,   35,   75,
           81,    8,  242,  277, 1315, 2586,  215,   20, 2303, 2024, 5104, 1614,
           75,   17,    4,  597,  332,  958, 3725]])
batch_word_ids: shape= torch.Size([1, 43]) 
 tensor([[ 1335,  1335,  1015,   167,   109,   109,     2,   291,   291,     1,
             0,     0,     3,   182,   182,  2850,  2850, 14197, 14197,     4,
             3,  2340,  2340,  2340,  2340,     4,   825,   825,   825,  3403,
          3403,    10,     0,     0,     0,     0,  1244,  1244,     2,     0,
             0, 12178, 12178]])
batch_ent_labels: shape= torch.Size([1, 43]) 
 tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2,
         2, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
batch_rel_labels: shape= torch.Size([1, 43, 49, 43]) 
 tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
selection_logits.shape =  torch.Size([1, 43, 49, 43])
selection_logits.shape =  torch.Size([1, 43, 49, 43])
mask.shape =  torch.Size([1, 43])
mask.unsqueeze(2).shape =  torch.Size([1, 43, 1])
mask.unsqueeze(1).shape =  torch.Size([1, 1, 43])
product:  torch.Size([1, 43, 1, 43])
selection_mask.shape =  torch.Size([1, 43, 49, 43])
selection_loss.shape =  torch.Size([1, 43, 49, 43])
selection_loss.shape =  torch.Size([90601])
selection_loss =  tensor(62700.1953, device='cuda:0', grad_fn=<SumBackward0>)
selection_loss =  tensor(1458.1440, device='cuda:0', grad_fn=<DivBackward0>)
selection_mask.sum =  tensor(43, device='cuda:0')
example:  Example: 
{
    "p_id": 7,
    "context": "南迦帕尔巴特峰，8125米",
    "raw_context": null,
    "text_word": [
        "南迦",
        "帕尔巴",
        "特峰",
        "，",
        "8125",
        "米"
    ],
    "bert_tokens": null,
    "entity_list": [
        "南迦帕尔巴特峰",
        "8125米"
    ],
    "gold_answer": [
        [
            "南迦帕尔巴特峰",
            "海拔",
            "8125米"
        ]
    ]
}
batch_char_ids: shape= torch.Size([1, 13]) 
 tensor([[ 104, 3185, 1461,  204,  508,  211,  531,    3,   28,    5,   12,   39,
          348]])
batch_word_ids: shape= torch.Size([1, 13]) 
 tensor([[  0,   0,   0,   0,   0,   0,   0,   1,   0,   0,   0,   0, 327]])
batch_ent_labels: shape= torch.Size([1, 13]) 
 tensor([[1, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2]])
batch_rel_labels: shape= torch.Size([1, 13, 49, 13]) 
 tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
selection_logits.shape =  torch.Size([1, 13, 49, 13])
selection_logits.shape =  torch.Size([1, 13, 49, 13])
mask.shape =  torch.Size([1, 13])
mask.unsqueeze(2).shape =  torch.Size([1, 13, 1])
mask.unsqueeze(1).shape =  torch.Size([1, 1, 13])
product:  torch.Size([1, 13, 1, 13])
selection_mask.shape =  torch.Size([1, 13, 49, 13])
selection_loss.shape =  torch.Size([1, 13, 49, 13])
selection_loss.shape =  torch.Size([8281])
selection_loss =  tensor(5895.0576, device='cuda:0', grad_fn=<SumBackward0>)
selection_loss =  tensor(453.4660, device='cuda:0', grad_fn=<DivBackward0>)
selection_mask.sum =  tensor(13, device='cuda:0')
example:  Example: 
{
    "p_id": 8,
    "context": "《身外身梦中梦》是连载于晋江文学城的一部原创类小说，作者是苍生笑",
    "raw_context": null,
    "text_word": [
        "《",
        "身外",
        "身梦中",
        "梦",
        "》",
        "是",
        "连载",
        "于",
        "晋江",
        "文学城",
        "的",
        "一部",
        "原创",
        "类",
        "小说",
        "，",
        "作者",
        "是",
        "苍生",
        "笑"
    ],
    "bert_tokens": null,
    "entity_list": [
        "身外身梦中梦",
        "苍生笑"
    ],
    "gold_answer": [
        [
            "身外身梦中梦",
            "作者",
            "苍生笑"
        ]
    ]
}
batch_char_ids: shape= torch.Size([1, 32]) 
 tensor([[   7,  232,  341,  232,  468,   17,  468,    8,    6,  137,  200,   20,
          611,  111,   47,   19,  248,    4,   15,   74,  239,  126,  284,   27,
           43,    3,   14,   50,    6, 1611,   26,  682]])
batch_word_ids: shape= torch.Size([1, 32]) 
 tensor([[    3,     0,     0,     0,     0,     0,   610,     4,     5,    50,
            50,    10,   308,   308,     0,     0,     0,     2,     0,     0,
           561,   561,   228,    41,    41,     1,    16,    16,     5, 15276,
         15276,  1132]])
batch_ent_labels: shape= torch.Size([1, 32]) 
 tensor([[0, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 1, 2, 2]])
batch_rel_labels: shape= torch.Size([1, 32, 49, 32]) 
 tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
selection_logits.shape =  torch.Size([1, 32, 49, 32])
selection_logits.shape =  torch.Size([1, 32, 49, 32])
mask.shape =  torch.Size([1, 32])
mask.unsqueeze(2).shape =  torch.Size([1, 32, 1])
mask.unsqueeze(1).shape =  torch.Size([1, 1, 32])
product:  torch.Size([1, 32, 1, 32])
selection_mask.shape =  torch.Size([1, 32, 49, 32])
selection_loss.shape =  torch.Size([1, 32, 49, 32])
selection_loss.shape =  torch.Size([50176])
selection_loss =  tensor(33229.6289, device='cuda:0', grad_fn=<SumBackward0>)
selection_loss =  tensor(1038.4259, device='cuda:0', grad_fn=<DivBackward0>)
selection_mask.sum =  tensor(32, device='cuda:0')

0it [00:00, ?it/s][Aexample:  Example: 
{
    "p_id": 1,
    "context": "查尔斯·阿兰基斯（charlesaránguiz），1989年4月17日出生于智利圣地亚哥，智利职业足球运动员，司职中场，效力于德国足球甲级联赛勒沃库森足球俱乐部",
    "raw_context": null,
    "text_word": [
        "查尔斯",
        "·",
        "阿兰",
        "基斯",
        "（",
        "charlesar",
        "á",
        "nguiz",
        "）",
        "，",
        "1989",
        "年",
        "4",
        "月",
        "17",
        "日出",
        "生于",
        "智利",
        "圣地亚哥",
        "，",
        "智利",
        "职业",
        "足球",
        "运动员",
        "，",
        "司职",
        "中场",
        "，",
        "效力",
        "于",
        "德国",
        "足球",
        "甲级联赛",
        "勒沃库森",
        "足球",
        "俱乐部"
    ],
    "bert_tokens": null,
    "entity_list": [
        "查尔斯·阿兰基斯",
        "圣地亚哥",
        "查尔斯·阿兰基斯",
        "1989年4月17日"
    ],
    "gold_answer": [
        [
            "查尔斯·阿兰基斯",
            "出生地",
            "圣地亚哥"
        ],
        [
            "查尔斯·阿兰基斯",
            "出生日期",
            "1989年4月17日"
        ]
    ]
}
p_ids:  tensor([0])
batch_char_ids:  tensor([[1125,  204,  163,   83,  437,  419,  321,  163,   54,  122,  209,   49,
           92,  132,   57,   85,   49,   92, 2927,   80,  263,  221,   69,  974,
           55,    3,    5,   13,   28,   13,   10,   46,   21,    5,   29,   32,
           16,   26,   20,  604,  301,  860,  133,  213,  734,    3,  604,  301,
          334,   37,  412,  186,  379,  138,   95,    3,   59,  334,   17,  217,
            3,  549,  198,   20,  219,   22,  412,  186,  986,  352,  281,  632,
         1110, 1306, 1368,  707,  412,  186, 1183,  136,   74]])
batch_word_ids:  tensor([[ 5085,  5085,  5085,    21,  6975,  6975, 21843, 21843,    14,     0,
             0,     0,     0,     0,     0,     0,     0,     0,  4769,     0,
             0,     0,     0,     0,    15,     1,   398,   398,   398,   398,
             7,    80,     8,   415,   415,   273,   273,    61,    61,  3445,
          3445, 12763, 12763, 12763, 12763,     1,  3445,  3445,   411,   411,
           188,   188,   180,   180,   180,     1,   643,   643,   735,   735,
             1,   193,   193,    10,   702,   702,   188,   188,  8287,  8287,
          8287,  8287, 41895, 41895, 41895, 41895,   188,   188,   515,   515,
           515]])
example:  Example: 
{
    "p_id": 2,
    "context": "《离开》是由张宇谱曲，演唱",
    "raw_context": null,
    "text_word": [
        "《",
        "离开",
        "》",
        "是",
        "由",
        "张宇",
        "谱曲",
        "，",
        "演唱"
    ],
    "bert_tokens": null,
    "entity_list": [
        "离开",
        "张宇",
        "离开",
        "张宇"
    ],
    "gold_answer": [
        [
            "离开",
            "歌手",
            "张宇"
        ],
        [
            "离开",
            "作曲",
            "张宇"
        ]
    ]
}
p_ids:  tensor([1])
batch_char_ids:  tensor([[  7, 690, 188,   8,   6,  41,  86, 577, 941,  44,   3,  23, 100]])
batch_word_ids:  tensor([[   3, 1186, 1186,    4,    5,   13, 3745, 3745,  460,  460,    1,   43,
           43]])
example:  Example: 
{
    "p_id": 3,
    "context": "《愤怒的唐僧》由北京吴意波影视文化工作室与优酷电视剧频道联合制作，故事以喜剧元素为主，讲述唐僧与佛祖打牌，得罪了佛祖，被踢下人间再渡九九八十一难的故事",
    "raw_context": null,
    "text_word": [
        "《",
        "愤怒",
        "的",
        "唐僧",
        "》",
        "由",
        "北京",
        "吴意波",
        "影视文化",
        "工作室",
        "与",
        "优酷",
        "电视剧",
        "频道",
        "联合",
        "制作",
        "，",
        "故事",
        "以",
        "喜剧",
        "元素",
        "为主",
        "，",
        "讲述",
        "唐僧",
        "与",
        "佛祖",
        "打牌",
        "，",
        "得罪",
        "了",
        "佛祖",
        "，",
        "被",
        "踢",
        "下",
        "人间",
        "再渡",
        "九九八十一",
        "难",
        "的",
        "故事"
    ],
    "bert_tokens": null,
    "entity_list": [
        "愤怒的唐僧",
        "北京吴意波影视文化工作室",
        "愤怒的唐僧",
        "吴意波"
    ],
    "gold_answer": [
        [
            "愤怒的唐僧",
            "出品公司",
            "北京吴意波影视文化工作室"
        ],
        [
            "愤怒的唐僧",
            "导演",
            "吴意波"
        ]
    ]
}
p_ids:  tensor([2])
batch_char_ids:  tensor([[   7, 2676, 1866,    4,  444, 2137,    8,   41,   84,  140,  380,  445,
          561,   76,   65,   47,  199,   70,   14,  669,  113,  657, 1384,   48,
           65,   35, 1031,  264,  281,  182,  180,   14,    3,  416,  157,  145,
          338,   35,  323, 1001,   51,   42,    3,  609,  674,  444, 2137,  113,
         1100,  499,  484,  642,    3,  212, 1327,   40, 1100,  499,    3,  295,
         2868,  194,   18,  335,  506, 1749,  605,  605,  629,  267,   15,  827,
            4,  416,  157]])
batch_word_ids:  tensor([[    3,  5222,  5222,     2,  4963,  4963,     4,    13,    74,    74,
             0,     0,     0,   727,   727,   727,   727,   816,   816,   816,
            27,  2786,  2786,    52,    52,    52,  1501,  1501,   164,   164,
           113,   113,     1,   151,   151,   116,   464,   464,  1964,  1964,
          1143,  1143,     1,   318,   318,  4963,  4963,    27,     0,     0,
             0,     0,     1, 16397, 16397,    12,     0,     0,     1,    78,
          9533,   233,  2026,  2026,     0,     0, 25410, 25410, 25410, 25410,
         25410,  2778,     2,   151,   151]])
example:  Example: 
{
    "p_id": 4,
    "context": "李治即位后，萧淑妃受宠，王皇后为了排挤萧淑妃，答应李治让身在感业寺的武则天续起头发，重新纳入后宫",
    "raw_context": null,
    "text_word": [
        "李治",
        "即位",
        "后",
        "，",
        "萧淑妃",
        "受宠",
        "，",
        "王皇后",
        "为了",
        "排挤",
        "萧淑妃",
        "，",
        "答应",
        "李治",
        "让",
        "身",
        "在",
        "感业",
        "寺",
        "的",
        "武则天",
        "续起",
        "头发",
        "，",
        "重新",
        "纳入",
        "后宫"
    ],
    "bert_tokens": null,
    "entity_list": [
        "李治",
        "萧淑妃",
        "萧淑妃",
        "李治"
    ],
    "gold_answer": [
        [
            "李治",
            "妻子",
            "萧淑妃"
        ],
        [
            "萧淑妃",
            "丈夫",
            "李治"
        ]
    ]
}
p_ids:  tensor([3])
batch_char_ids:  tensor([[ 101,  462,  898,  153,   82,    3, 1137, 1330, 1009,  528, 1380,    3,
           81,  521,   82,   51,   40, 1111, 3203, 1137, 1330, 1009,    3, 1741,
          612,  101,  462,  423,  232,   25,  375,   37, 1720,    4,  311,  857,
           89,  625,  215,  472,   73,    3,  234,  110,  673,  210,   82,  807]])
batch_word_ids:  tensor([[ 6798,  6798,  6303,  6303,    89,     1,     0,     0,     0, 21875,
         21875,     1, 20569, 20569, 20569,   438,   438, 23771, 23771,     0,
             0,     0,     1,  6896,  6896,  6798,  6798,   106,  4210,     9,
             0,     0,  4284,     2,  1482,  1482,  1482,     0,     0,  6635,
          6635,     1,  1433,  1433, 15573, 15573,  3265,  3265]])
example:  Example: 
{
    "p_id": 5,
    "context": "《工业4.0》是2015年机械工业出版社出版的图书，作者是（德）阿尔冯斯·波特霍夫，恩斯特·安德雷亚斯·哈特曼",
    "raw_context": null,
    "text_word": [
        "《",
        "工业",
        "4.0",
        "》",
        "是",
        "2015",
        "年",
        "机械",
        "工业",
        "出版社",
        "出版",
        "的",
        "图书",
        "，",
        "作者",
        "是",
        "（",
        "德",
        "）",
        "阿尔",
        "冯斯",
        "·",
        "波特",
        "霍夫",
        "，",
        "恩斯特",
        "·",
        "安德雷",
        "亚斯",
        "·",
        "哈特曼"
    ],
    "bert_tokens": null,
    "entity_list": [
        "工业4.0",
        "恩斯特·安德雷亚斯·哈特曼",
        "工业4.0",
        "阿尔冯斯·波特霍夫",
        "工业4.0",
        "机械工业出版社"
    ],
    "gold_answer": [
        [
            "工业4.0",
            "作者",
            "恩斯特·安德雷亚斯·哈特曼"
        ],
        [
            "工业4.0",
            "作者",
            "阿尔冯斯·波特霍夫"
        ],
        [
            "工业4.0",
            "出版社",
            "机械工业出版社"
        ]
    ]
}
p_ids:  tensor([4])
batch_char_ids:  tensor([[   7,   70,   37,   46,  400,    9,    8,    6,   12,    9,    5,   39,
           10,  339, 1025,   70,   37,   16,   30,   71,   16,   30,    4,   88,
           53,    3,   14,   50,    6,   54,  219,   55,  437,  204,  910,  163,
           83,  561,  211,  875,  394,    3,  571,  163,  211,   83,  174,  219,
          589,  213,  163,   83,  663,  211,  905]])
batch_word_ids:  tensor([[    3,   313,   313,     0,     0,     0,     4,     5,   194,   194,
           194,   194,     7,   667,   667,   313,   313,    39,    39,    39,
            20,    20,     2,    25,    25,     1,    16,    16,     5,    14,
           978,    15, 14630, 14630,     0,     0,    21, 13565, 13565, 29992,
         29992,     1, 32112, 32112, 32112,    21, 18568, 18568, 18568, 20024,
         20024,    21,     0,     0,     0]])
example:  Example: 
{
    "p_id": 6,
    "context": "周佛海被捕入狱之后，其妻杨淑慧散尽家产请蒋介石枪下留人，于是周佛海从死刑变为无期，不过此人或许作恶多端，改判没多久便病逝于监狱，据悉是心脏病发作",
    "raw_context": null,
    "text_word": [
        "周佛海",
        "被捕",
        "入狱",
        "之后",
        "，",
        "其妻",
        "杨淑",
        "慧散",
        "尽",
        "家产",
        "请",
        "蒋介石",
        "枪下",
        "留人",
        "，",
        "于是",
        "周佛海",
        "从",
        "死刑",
        "变为",
        "无期",
        "，",
        "不过",
        "此人",
        "或许",
        "作恶多端",
        "，",
        "改判",
        "没多久",
        "便",
        "病逝",
        "于",
        "监狱",
        "，",
        "据悉",
        "是",
        "心脏病",
        "发作"
    ],
    "bert_tokens": null,
    "entity_list": [
        "周佛海",
        "杨淑慧",
        "杨淑慧",
        "周佛海"
    ],
    "gold_answer": [
        [
            "周佛海",
            "妻子",
            "杨淑慧"
        ],
        [
            "杨淑慧",
            "丈夫",
            "周佛海"
        ]
    ]
}
p_ids:  tensor([5])
batch_char_ids:  tensor([[ 242, 1100,   98,  295, 1945,  210, 2077,   75,   82,    3,  250,  646,
          265, 1330,  831, 1432, 1184,   52,  195, 1039,  948,  289,  463, 1769,
          194,  810,   18,    3,   20,    6,  242, 1100,   98,  290,  809, 1664,
          630,   51,  282,  307,    3,   63,  226,  492,   18, 1098,  518,   14,
         1479,  158, 1367,    3,  440, 1462,  427,  158,  973,  994, 1033, 1656,
           20,  452, 2077,    3,  650, 1431,    6,  156, 2753, 1033,   73,   14]])
batch_word_ids:  tensor([[28461, 28461, 28461, 13043, 13043, 13661, 13661,   214,   214,     1,
         12498, 12498,     0,     0,     0,     0,  3940, 23515, 23515,  1022,
          2846,  2846,  2846,     0,     0,     0,     0,     1,  1105,  1105,
         28461, 28461, 28461,   129, 28817, 28817, 25736, 25736,     0,     0,
             1,   557,   557, 12491, 12491,  2675,  2675,     0,     0,     0,
             0,     1,     0,     0, 15403, 15403, 15403,   695,  6329,  6329,
            10,  5831,  5831,     1,  2503,  2503,     5, 18654, 18654, 18654,
         23869, 23869]])
example:  Example: 
{
    "p_id": 7,
    "context": "《李烈钧自述》是2011年11月1日人民日报出版社出版的图书，作者是李烈钧",
    "raw_context": null,
    "text_word": [
        "《",
        "李烈钧",
        "自述",
        "》",
        "是",
        "2011",
        "年",
        "11",
        "月",
        "1",
        "日",
        "人民日报",
        "出版社",
        "出版",
        "的",
        "图书",
        "，",
        "作者",
        "是",
        "李烈钧"
    ],
    "bert_tokens": null,
    "entity_list": [
        "李烈钧自述",
        "李烈钧",
        "李烈钧自述",
        "人民日报出版社"
    ],
    "gold_answer": [
        [
            "李烈钧自述",
            "作者",
            "李烈钧"
        ],
        [
            "李烈钧自述",
            "出版社",
            "人民日报出版社"
        ]
    ]
}
p_ids:  tensor([6])
batch_char_ids:  tensor([[   7,  101, 1138, 1436,  125,  674,    8,    6,   12,    9,    5,    5,
           10,    5,    5,   21,    5,   32,   18,  164,   32,  661,   16,   30,
           71,   16,   30,    4,   88,   53,    3,   14,   50,    6,  101, 1138,
         1436]])
batch_word_ids:  tensor([[   3,    0,    0,    0, 9541, 9541,    4,    5,  108,  108,  108,  108,
            7,   91,   91,    8,   23,   18, 8063, 8063, 8063, 8063,   39,   39,
           39,   20,   20,    2,   25,   25,    1,   16,   16,    5,    0,    0,
            0]])
example:  Example: 
{
    "p_id": 8,
    "context": "除演艺事业外，李冰冰热心公益，发起并亲自参与多项环保慈善活动，积极投身其中，身体力行担起了回馈社会的责任于02年出演《少年包青天》，进入大家视线",
    "raw_context": null,
    "text_word": [
        "除",
        "演艺事业",
        "外",
        "，",
        "李冰冰",
        "热心",
        "公益",
        "，",
        "发起",
        "并",
        "亲自",
        "参与",
        "多项",
        "环保",
        "慈善",
        "活动",
        "，",
        "积极",
        "投身",
        "其中",
        "，",
        "身体力行",
        "担起",
        "了",
        "回馈",
        "社会",
        "的",
        "责任",
        "于",
        "02",
        "年",
        "出演",
        "《",
        "少年",
        "包青天",
        "》",
        "，",
        "进入",
        "大家",
        "视线"
    ],
    "bert_tokens": null,
    "entity_list": [
        "少年包青天",
        "李冰冰"
    ],
    "gold_answer": [
        [
            "少年包青天",
            "主演",
            "李冰冰"
        ]
    ]
}
p_ids:  tensor([7])
batch_char_ids:  tensor([[1149,   23,  251,  157,   37,  341,    3,  101,  722,  722,  429,  156,
           38, 1296,    3,   73,  215,  287,  383,  125,  227,  113,  158,  856,
          710,  522, 1387, 1020,  421,  138,    3,  628,  695,  598,  232,  250,
           17,    3,  232,  254,  198,  105,  569,  215,   40,  454, 3973,   71,
          121,    4,  909,  147,   20,    9,   12,   10,   16,   23,    7,  313,
           10,  712,  261,   89,    8,    3,  384,  210,   24,   52,   65,  588]])
batch_word_ids:  tensor([[ 4281,  6406,  6406,  6406,  6406,  1257,     1,  7540,  7540,  7540,
         14432, 14432,  3250,  3250,     1,  3186,  3186,    86,  1134,  1134,
           733,   733,     0,     0,  1736,  1736,  5651,  5651,   981,   981,
             1,  3084,  3084, 13148, 13148,   248,   248,     1,     0,     0,
             0,     0, 28218, 28218,    12, 23562, 23562,   624,   624,     2,
          5046,  5046,    10,   642,   642,     7,   162,   162,     3,   646,
           646,  5584,  5584,  5584,     4,     1,   485,   485,   257,   257,
          4229,  4229]])

                  [A============================================
dev/entity_em: 0,	entity_pred_num&entity_gold_num: 207	21 
dev/entity_f1: 0.0, 	entity_precision: 0.0,	entity_recall: 0.0 
============================================
dev/em: 1e-10,	pre&gold: 377.0000000001	16.000000000100002 
dev/f1: 5.089058524170439e-11, 	Precision: 2.652519893898501e-11,	Recall: 6.249999999960937e-10 
